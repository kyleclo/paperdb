{"query": "transformer architecture for sequence modeling", "document_id": "paper_001", "relevance": 1}
{"query": "pre-training language models with masked tokens", "document_id": "paper_002", "relevance": 1}
{"query": "few-shot learning with large language models", "document_id": "paper_003", "relevance": 1}
{"query": "convolutional neural networks for image classification", "document_id": "paper_004", "relevance": 1}
{"query": "combining retrieval with text generation", "document_id": "paper_005", "relevance": 1}
{"query": "residual connections for deep networks", "document_id": "paper_006", "relevance": 1}
{"query": "attention mechanism without recurrence", "document_id": "paper_001", "relevance": 1}
